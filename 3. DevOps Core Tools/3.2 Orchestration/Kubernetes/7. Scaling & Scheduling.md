---
title: "7. Scaling & Scheduling"
parent: "‚Ä¢ Kubernetes"
grand_parent: 3. DevOps Core Tools
nav_order: 7
has_children: true
---
Topics Included - HPA, VPA, Cluster Autoscaling, Resource Requests/Limits

Sub-Sections / Deep-Dive Areas - Node Affinity, Taints/Tolerations, Probes (Liveness/Readiness)

---


‚úÖ Scaling ‚Üí Automatically or manually increasing or decreasing the number of pods or resources based on workload demand.

‚úÖ Scheduling ‚Üí Deciding which node a pod should run on by matching resource needs and placement rules.

‚úÖ HPA ‚Üí Horizontal scaling (replicas increase/decrease)

‚úÖ VPA ‚Üí Vertical scaling (increase CPU/Memory of pod)

‚úÖ Node Affinity

‚úÖ Taints & Tolerations

‚úÖ Resource Quotas

‚úÖ Limits & Requests

‚úÖ Probes

======================================

# ‚úÖ **Kubernetes Resource Quotas**

## üìå What are Resource Quotas?

Resource Quotas limit how much compute power, memory, storage, and number of objects a **namespace** can use.

They prevent:

* One user/app consuming entire cluster
* Over-provisioning of CPU/Memory
* Uncontrolled creation of Pods, PVCs, Services, etc.

### Why Resource Quotas Are Used?

‚úî Shared multi-team clusters
‚úî Cost control
‚úî Fair resource distribution
‚úî Enforcing limits & best practices

---

### 1Ô∏è‚É£ **Create Namespace**

`namespace.yml`

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nginx
```

Apply:

```bash
kubectl apply -f namespace.yml
```

---

### 2Ô∏è‚É£ **Create Resource Quota**

`resourceQuota.yml`

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: nginx-quota
  namespace: nginx
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    pods: "10"
    requests.storage: 5Gi
    persistentvolumeclaims: "5"
```

Apply:

```bash
kubectl apply -f resourceQuota.yml
```

Check quota:

```bash
kubectl describe quota -n nginx
```

---

### 3Ô∏è‚É£ **Create Persistent Volume (PV)**

`persistentVolume.yml`

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

Apply:

```bash
kubectl apply -f persistentVolume.yml
```

If error shown:

> persistentvolume/local-pv unchanged
> Means ‚Üí PV already exists.

To delete and recreate:

```bash
kubectl get pv
kubectl delete pv local-pv
kubectl apply -f persistentVolume.yml
```

---

### 4Ô∏è‚É£ **Persistent Volume Claim (PVC)**

`persistentVolumeClaim.yml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
  namespace: nginx
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Apply:

```bash
kubectl apply -f persistentVolumeClaim.yml
```

---

### 5Ô∏è‚É£ **Deployment with Requests & Limits (Quota Required)**

`deployment.yml`

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nginx-deployment
  namespace: nginx

spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx

  template:
    metadata:
      name: nginx-dep-pod
      labels:
        app: nginx

    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi

        volumeMounts:
        - mountPath: /var/www/html
          name: my-volume

      volumes:
      - name: my-volume
        persistentVolumeClaim:
          claimName: local-pvc
```

Apply:

```bash
kubectl apply -f deployment.yml
```

---

### 6Ô∏è‚É£ **Verification Commands**

### List pods:

```bash
kubectl get pods -n nginx
kubectl get pods -n nginx -o wide
```

### Verify Deployment Applied Correct Requests & Limits:

```bash
kubectl describe pod <pod-name> -n nginx
```

Sample Output:

```
Limits:
  cpu:     200m
  memory:  256Mi
Requests:
  cpu:      100m
  memory:   128Mi
```

---

### 7Ô∏è‚É£ **Resource Quota Validation**

To verify quota enforcement:

```bash
kubectl describe quota nginx-quota -n nginx
```

You will see:

```
Used:
  requests.cpu: 200m
  limits.cpu:   400m
  pods:         2
```

This confirms your Deployment is counted against the Namespace Quota.

---

==============================


# ‚úÖ  **Probes**

1) Liveness Probe, 
2) Readiness Probe, 
3) startup Probe



## üß™ Kubernetes Probes Overview

**Probes** are used by the kubelet to check the health and readiness of containers running inside Pods. There are three types:

| Probe Type       | Purpose                                      | When It Runs                         |
|------------------|----------------------------------------------|--------------------------------------|
| **Liveness Probe** | Checks if the container is *alive* (running) | Periodically after startup           |
| **Readiness Probe** | Checks if the container is *ready* to serve traffic | Periodically after startup           |
| **Startup Probe** | Checks if the container has *started successfully* | Only during startup phase            |

---

## üîÅ 1. Liveness Probe

- **Purpose:** Detects if the container is stuck or dead
- **Action:** If it fails, Kubernetes **restarts** the container
- **Use Case:** Long-running apps that might hang (e.g., memory leaks, deadlocks)

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
```

---

## ‚úÖ 2. Readiness Probe

- **Purpose:** Checks if the container is ready to receive traffic
- **Action:** If it fails, the Pod is **removed from Service endpoints**
- **Use Case:** Apps that need warm-up time or depend on external services

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```

---

## üöÄ 3. Startup Probe

- **Purpose:** Gives slow-starting containers more time before liveness kicks in
- **Action:** If it fails, container is killed and restarted
- **Use Case:** Legacy apps or heavy initialization logic

```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```

> Once the **startupProbe** succeeds, **livenessProbe** and **readinessProbe** take over.

---

## üß† Probe Types (Methods)

All probes support these methods:

- `httpGet`: Call an HTTP endpoint
- `tcpSocket`: Check if a TCP port is open
- `exec`: Run a command inside the container

---

## üß∞ Best Practices

- Use **startupProbe** for slow apps to avoid false liveness failures
- Keep **readinessProbe** lightweight ‚Äî avoid heavy DB calls
- Tune `initialDelaySeconds`, `periodSeconds`, and `failureThreshold` based on app behavior

==============================



# ‚úÖ **Taints & Tolerations**

## **1Ô∏è‚É£ What Are Taints & Tolerations?**

* **Taints** ‚Üí Node-level restrictions
* **Tolerations** ‚Üí Pod-level permissions
* **Goal:** Prevent pods from being scheduled on specific nodes unless they ‚Äútolerate‚Äù the taint.

---

# **2Ô∏è‚É£ Taint All Worker Nodes**

### **Check nodes**

```bash
sudo kubectl get nodes
```

### **Apply taints**

```bash
sudo kubectl taint node tws-cluster-worker prod=true:NoSchedule
sudo kubectl taint node tws-cluster-worker2 prod=true:NoSchedule
sudo kubectl taint node tws-cluster-worker3 prod=true:NoSchedule
```

---

## **3Ô∏è‚É£ Create Namespace & Pod**

```bash
sudo kubectl apply -f namespace.yml
sudo kubectl apply -f pod.yml
```

### **Check Pod**

```bash
sudo kubectl get pod -n nginx
```

Output:

```
0/1 Pending
```

**Reason:** All nodes are tainted ‚Üí Pod cannot schedule.

---

## **4Ô∏è‚É£ Check Why Pod is Pending**

```bash
sudo kubectl describe pod nginx-pod -n nginx
```

Output will show:

```
0/4 nodes available:
1 had control-plane taint
3 had prod=true taint
```

---

## **5Ô∏è‚É£ Remove Taint (Un-taint a Node)**

To allow pod scheduling, remove taint using **trailing hyphen (-)**:

```bash
sudo kubectl taint node tws-cluster-worker2 prod=true:NoSchedule-
```

### **Now pod will run**

```bash
sudo kubectl get pod -n nginx
```

---

# **6Ô∏è‚É£ Important Concept**

‚ùó **If pod is already scheduled on a node ‚Üí it will continue running even if node becomes tainted.**
But new pods cannot be scheduled.

---

## **7Ô∏è‚É£ Re-apply Taint Again**

```bash
sudo kubectl taint node tws-cluster-worker2 prod=true:NoSchedule
```

Delete the pod:

```bash
sudo kubectl delete -f pod.yml
```

Now pod cannot run unless tolerations are added.

---

## **8Ô∏è‚É£ Add Tolerations in Pod YAML**

### **pod.yml**

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: nginx-pod
  namespace: nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  tolerations:
  - key: "prod"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
```

---

## **9Ô∏è‚É£ Apply Pod With Tolerations**

```bash
sudo kubectl apply -f pod.yml
sudo kubectl get pod -n nginx
```

‚úî Pod will run even though the node is tainted.

---

### **üîü Summary for Interview**

| Feature                                 | Meaning                                          |
| --------------------------------------- | ------------------------------------------------ |
| **Taint**                               | Restricts nodes from accepting pods              |
| **Toleration**                          | Allows pod to bypass taint                       |
| **Command to taint**                    | `kubectl taint node <node> key=value:NoSchedule` |
| **Remove taint**                        | Add hyphen: `key=value:NoSchedule-`              |
| **Pod keeps running even after taint?** | Yes, until deleted                               |


==============================


---

# ‚úÖ **Kubernetes AutoScaling **

---

### **1Ô∏è‚É£ What is AutoScaling?**

**AutoScaling** means Kubernetes automatically increases or decreases:

* **Pods** (HPA)
* **CPU/Memory settings** (VPA)
* **Pods based on events** (KEDA)

AutoScaling improves:
‚úî Performance
‚úî Cost optimization
‚úî High availability

---

## **2Ô∏è‚É£ Types of AutoScaling in Kubernetes**

### **A. HPA ‚Äì Horizontal Pod Autoscaler**

* Pod count automatically increases or decreases based on **CPU/Memory metrics**.
* Example:
  ‚ûù When CPU > 80%, replicas increase.
  ‚ûù When CPU < 20%, replicas decrease.

**HPA uses:**
‚úî CPU
‚úî Memory
‚úî Custom metrics

---

### **B. VPA ‚Äì Vertical Pod Autoscaler**

* Automatically adjusts **requests & limits**.
* Used for **Stateful applications**, like:

  * MySQL
  * MongoDB
  * Kafka
  * Cassandra

Why Stateful?
‚û° Stateful apps cannot scale horizontally easily (data consistency issue).

---

### **C. KEDA ‚Äì Kubernetes Event-Driven Autoscaler**

* Autoscaling based on **events**, not CPU/Memory.
* Works with 60+ event sources:

  * Kafka messages
  * RabbitMQ queue
  * Azure queue
  * AWS SQS
  * Prometheus queries
  * Cron

**KEDA = HPA + VPA + Event Scaling**

---

# **3Ô∏è‚É£ Metrics Server ‚Äì MOST IMPORTANT**

HPA requires metrics.
Without Metrics Server:

```
error: Metrics API not available
```

To install Metrics Server (Kind cluster or bare metal):

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### Now edit deployment

```bash
kubectl -n kube-system edit deployment metrics-server
```

Add under **spec ‚Üí containers ‚Üí args**:

```bash
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
```

Restart deployment:

```bash
kubectl -n kube-system rollout restart deployment metrics-server
```

### Verify:

```bash
kubectl get pods -n kube-system
kubectl top nodes
kubectl top pod -n nginx
```

---

## **4Ô∏è‚É£ HPA Practical (Apache HTTPD Example)**

### **Step 1 ‚Äî Create Namespace**

`namespace.yml`

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: apache
```

```bash
kubectl apply -f namespace.yml
```

---

### **Step 2 ‚Äî Create Deployment**

`deployment.yml`

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: apache-deployment
  namespace: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
```

```bash
kubectl apply -f deployment.yml
```

---

### **Step 3 ‚Äî Create Service**

`service.yml`

```yaml
kind: Service
apiVersion: v1
metadata:
  name: apache-service
  namespace: apache
spec:
  selector:
    app: apache
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
```

```bash
kubectl apply -f service.yml
```

---

### **Access Service Inside Cluster**

```bash
curl http://apache-service.apache.svc.cluster.local
```

---

### **Port Forward for Browser Access**

```bash
kubectl port-forward service/apache-service -n apache 82:80 --address=0.0.0.0
```

Access:

```
http://<node-ip>:82/
```

---

## **5Ô∏è‚É£ Create HPA**

`hpa.yml`

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: apache-hpa
  namespace: apache
spec:
  scaleTargetRef:
    kind: Deployment
    name: apache-deployment
    apiVersion: apps/v1
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 5
```

Apply:

```bash
kubectl apply -f hpa.yml
```

Check:

```bash
kubectl get hpa -n apache
```

---

## **6Ô∏è‚É£ Generate Load to Trigger HPA**

Run load generator:

```bash
kubectl run -i --tty load-generator --image=busybox -n apache -- /bin/sh
```

Inside container:

```bash
while true; do wget -q -O- http://apache-service.apache.svc.cluster.local; done
```

---

### **Check if Pods Scale**

```bash
kubectl get hpa -n apache
kubectl get pods -n apache
```

Expected output:

```
apache-hpa  cpu: 23%/5%   Replicas: 5
```

---

### **7Ô∏è‚É£ Stop Autoscaling**

```bash
kubectl delete -f hpa.yml
```

---

## üéØ **Interview Summary (Short Notes)**

| AutoScaler         | Works On           | Best For                                |
| ------------------ | ------------------ | --------------------------------------- |
| **HPA**            | CPU/Memory metrics | Stateless apps (Nginx, Apache, Node.js) |
| **VPA**            | Requests/Limits    | Stateful apps (MySQL, MongoDB)          |
| **KEDA**           | Events             | Queue-based, message-driven apps        |
| **Metrics Server** | Collects Metrics   | Required for HPA                        |



==============================

---

## ‚úÖ **Vertical Pod Autoscaler (VPA)**

---

### **1Ô∏è‚É£ What is VPA?**

**Vertical Pod Autoscaler (VPA)** automatically adjusts:

* **CPU Requests**
* **Memory Requests**
* **CPU Limits**
* **Memory Limits**

VPA is mainly used for **Stateful applications**, such as:

* MySQL
* MongoDB
* PostgreSQL
* Kafka / Zookeeper
* Elasticsearch

Why?
‚û° Stateful apps **do not scale horizontally easily**, so VPA adjusts resources automatically.

---

### **2Ô∏è‚É£ VPA Components**

Installing VPA creates 3 components:

1. **VPA Recommender**
   üìå Suggests optimal CPU/Memory

2. **VPA Updater**
   üìå Restarts pods if required for resource changes

3. **VPA Admission Controller**
   üìå Injects new CPU/Memory requests into pod specs

---

### **3Ô∏è‚É£ Pre-requisites (From Kubernetes Official GitHub)**

Official repo:
[https://github.com/kubernetes/autoscaler.git](https://github.com/kubernetes/autoscaler.git)

### Clone the VPA Repo

```bash
git clone https://github.com/kubernetes/autoscaler.git
```

### Move to VPA directory

```bash
cd autoscaler/vertical-pod-autoscaler
```

### Checkout release version

```bash
git checkout origin/vpa-release-1.0
```

### Install VPA

```bash
sudo ./hack/vpa-up.sh
```

This script installs:

‚úî VPA Recommender
‚úî VPA Updater
‚úî Admission Controller

---

## **4Ô∏è‚É£ Create a VPA Object**

`vpa.yml`

```yaml
kind: VerticalPodAutoscaler
apiVersion: autoscaling.k8s.io/v1
metadata:
  name: apache-vpa
  namespace: apache

spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: apache-deployment

  updatePolicy:
    updateMode: "Auto"
```

---

## **5Ô∏è‚É£ Apply & Verify**

### Apply VPA

```bash
kubectl apply -f vpa.yml

### Check VPA

kubectl get vpa -n apache

### Check Services

kubectl get svc -n apache

### Check Pods

kubectl get pods -n apache

### Watch VPA recommendations

watch kubectl get vpa -n apache
```

---

### **6Ô∏è‚É£ Verify Pod Resource Changes**

```bash
Check pod CPU/Memory usage:

kubectl top pod -n apache

Watch live changes:

watch kubectl get vpa -n apache
```

What will change?

* Requests will increase automatically
* Limits will update based on usage
* Pod may restart (Auto mode)

---

## **7Ô∏è‚É£ VPA Modes (Very Important)**

| Mode        | Meaning                                            | Restarts Pods? |
| ----------- | -------------------------------------------------- | -------------- |
| **Off**     | Only gives recommendations                         | ‚ùå No           |
| **Initial** | Applies recommended resources only at pod creation | ‚ùå No           |
| **Auto**    | Automatically updates requests/limits              | ‚úî Yes          |

In your setup:

```
updateMode: Auto
```

---

### **8Ô∏è‚É£ When to Use VPA?**

Use VPA when:

‚úî Workload is stateful
‚úî Application cannot scale horizontally
‚úî Resource usage changes frequently
‚úî You want CPU/Memory tuning automatically

Do NOT use VPA with HPA (for CPU/Memory) on the **same workload** ‚Üí conflict.

---

### **9Ô∏è‚É£ Node Affinity ‚Äî Clean Notes**

Node Affinity means:

‚û° **‚ÄúMujhe ye pod sirf specific node par hi chalana hai.‚Äù**

It works like:

* Node Selector (simple)
* Node Affinity (advanced rule based)

### Example

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: workload
          operator: In
          values:
          - database
```

Meaning:
‚úî Pod must run on a node that has this label:

```
workload=database
```

---

